{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "seed = 1\n",
    "result_folder = 'results'\n",
    "score_file = os.path.join(result_folder, 'scores.txt')\n",
    "test_file = os.path.join(result_folder, 'test.tsv')\n",
    "os.makedirs(result_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename, splitter=\"\\t\"):\n",
    "    data = []\n",
    "    sentence = []\n",
    "    tags = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            if not line.isspace():\n",
    "                word, tag = line.split(splitter)\n",
    "                sentence.append(word)\n",
    "                tags.append(tag.strip())\n",
    "            else:\n",
    "                data.append((sentence, tags))\n",
    "                sentence = []\n",
    "                tags = []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = read_dataset(\"data/train.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = read_dataset(\"data/test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "\n",
    "\n",
    "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "            \n",
    "for sent, tags in test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
    "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
    "\n",
    "\n",
    "tag_to_ix = {\n",
    "    \"O\": 0,\n",
    "    \"B-Object\": 1,\n",
    "    \"I-Object\": 2,\n",
    "    \"B-Aspect\": 3,\n",
    "    \"I-Aspect\": 4,\n",
    "    \"B-Predicate\": 5,\n",
    "    \"I-Predicate\": 6\n",
    "}  # Assign each tag with a unique index\n",
    "\n",
    "idx_to_tag = dict(map(reversed, tag_to_ix.items()))\n",
    "\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), 0, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class Tagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim, dropout=0)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=4, dim_feedforward=2 * embedding_dim, dropout=0)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "        #self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Sequential(nn.Linear(embedding_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, tagset_size))\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        embeds = self.pos_encoder(embeds)\n",
    "        #lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        out = self.transformer(embeds.view(len(sentence), 1, -1))\n",
    "        #out = self.lstm(out)[0]\n",
    "        tag_space = self.hidden2tag(out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Tagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 20/20 [07:13<00:00, 21.65s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "for epoch in tqdm(range(20)):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "also O\n",
      ", O\n",
      "i O\n",
      "have O\n",
      "recently O\n",
      "discovered O\n",
      "advil B-Object\n",
      "liquigels O\n",
      "work O\n",
      "much O\n",
      "better B-Predicate\n",
      "and O\n",
      "faster B-Predicate\n",
      "for O\n",
      "a O\n",
      "headache B-Aspect\n",
      "than O\n",
      "regular O\n",
      "ibuprofen B-Object\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    tags = [idx_to_tag[int(i)] for i in tag_scores.argmax(dim=-1)]\n",
    "    \n",
    "    for i, y in zip(training_data[0][0], tags):\n",
    "        print(i, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████| 283/283 [00:00<00:00, 1112.40it/s]\n"
     ]
    }
   ],
   "source": [
    "with open(test_file, \"w\") as w:\n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(test_data):\n",
    "            inputs = prepare_sequence(sentence[0], word_to_ix)\n",
    "            tag_scores = model(inputs)\n",
    "            tags = [idx_to_tag[int(i)] for i in tag_scores.argmax(dim=-1)]\n",
    "            for i, y in zip(sentence[0], tags):\n",
    "                w.write(f\"{i}\\t{y}\\n\")\n",
    "            w.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_average_strict: 0.437729\n",
      "f1_aspect_strict: 0.128834\n",
      "f1_object_strict: 0.354149\n",
      "f1_predicate_strict: 0.812500\n",
      "f1_average: 0.452591\n",
      "f1_aspect: 0.158998\n",
      "f1_object: 0.360194\n",
      "f1_predicate: 0.835016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluation.evaluate_f1_partial import main\n",
    "\n",
    "main('data/test.tsv', test_file, score_file)\n",
    "    \n",
    "with open(score_file, \"r\") as f:\n",
    "    print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
